{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 피처정규화가 중요한 이유?\n",
    " - 모든 특징을 대략적으로 비슷하나 수치 구간 내로 이동시킬 수 있음.\n",
    " - 종류\n",
    "> 선형함수 정규화[x - min(x) / max(x) - min(x)]<br>\n",
    "> 표준 정규화[x-mean(x)/std(x)], mean - 평균 / std - 표준편차\n",
    " - 경사하강법의 경우 더 빠르게 최적의 해를 찾을 수 있음.(선형회귀, 로지스틱회구, SVM, NN 등)\n",
    "> 의사결정 나무 계열의 모델은 정규화가 필요없음\n",
    "\n",
    "2. 범주형 피쳐(순서형, 원-핫, 이진)\n",
    "\n",
    "5. 텍스트 표현 모델\n",
    ">> $ TF-IDF(t,d) = TF(t,d) * IDF(t) $ <br>\n",
    ">> $ IDF(t) = log ( 총 문장 수 \\div ( 단어 t를 포함하는 문장 수 + 1) $)\n",
    "\n",
    " - 워드 임베딩 : 저차원 공간(일반적으로 K = 50 ~ 300)상의 고밀도 벡터(dense vector)에 각 단어를 투영시키는 것\n",
    "  > N개의 단어와 K차원일 경우 워드 임베딩은 $N \\times K$ 차원의 행렬을 사용\n",
    "\n",
    "6. Word2Vec - 2013년 구글에서 발표한 워드 임베딩 모델\n",
    " - 워드투벡터의 2가지 방식\n",
    "  > CBOW방식 : 주변단어를 통하여 빈칸의 단어를 예측<br>\n",
    "  > Skip-gram방식 : 빈칸의 단어를 통하여 주변 단어를 예측\n",
    " - 토픽모델링과 워드 임베딩의 차이 : 모델의 차이이다. 토픽모델링은 확률 그래프 모델에 기반을 둔 생성(Generative) 모델이며, 워드 임베딩은 신경망 형식으로 학습하는 고밀도 벡터 표현방식\n",
    "\n",
    "7. 이미지 데이터가 부족할 때는 어떻게 처리해야 할까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 평가 지표의 한계\n",
    " - 문제 : 분류 문제, 배열 문제, 회귀 문제\n",
    " - 평가지표 : 정확도(Accuracy), 정밀도(Precision), 재현율(Recall), 평균제곱근오차(RMSE)\n",
    " - 정확도의 한계\n",
    " > 라벨값이 불균형한 데이터의 경우 객관적인 지표의 역할을 할 수 없다.<br>\n",
    " > Multi-Label의 경우 평균 정확도(각 클래스 샘플의 정확도의 산술평균)를 사용하여 해결 할 수 있다.<br>\n",
    " \n",
    " - 정밀도와 재현율의 균형\n",
    " > * 정밀도(Precision) : 분류기가 양성 샘플이라고 분류한 것 중에서 실제 양성 샘플인 것의 비율<br>\n",
    " > - 재현율(Recall) : 실제 양성 샘플인 것 중에서 분류기가 정확히 분류해 낸 양성 샘플의 비율<br>\n",
    " > - P-R곡선(x:Recall, y:Precision)<br>\n",
    " > - F1 score<br>\n",
    " >> F1 = $2 \\ \\times \\ precision \\ \\times \\ recall \\over precision \\ + \\ recall$<br>\n",
    " \n",
    " - 평균제곱근오차의 예외\n",
    " > - RMSE공식<br>\n",
    " >> RMSE = $ \\sum_{i=1}^n \\ (y_i - \\widehat{y}_i)^2  \\over n $ <br>\n",
    " > - 문제 : 특이점(Outlier)이 존재할 경우, RMSE의 지표값이 매우 높아지게 된다.<br>\n",
    " > - 해결방법<br>\n",
    " >> 특이점이 노이즈일 경우, 전처리를 통하여 해결<br>\n",
    " >> 노이즈가 아닐 경우, 모델의 성능 자체를 향상시켜야됨(특이점을 예측할 수 있도록)<br>\n",
    " >> 평균절대비오차(Mean Absolute Percent Error)지표 사용 : 각 포인트의 오차들을 정규화하여 특이점에 의해 발생하는 절대 오차의 영향력을 낮춤<br>\n",
    " >>> MAPE = $ \\sum_{i=1}^n \\left| \\ (y_i - \\widehat{y}_i) \\over y_i \\right| \\ \\times$ $ 100 \\over n $\n",
    " \n",
    "2. ROC곡선(Receiver Operating Characteristic Curve) : 이진분류기에서 자주 보이고 광범위하게 응용되는 분류기\n",
    " - 가로축은 거짓 양성 비율(FPR : False Positive Rate), 세로축은 실제 양성 비율(TPR : True Positive Rate)\n",
    " - ROC곡선과 P-R곡선 차이 : ROC는 모델 세트에 상관없이 안정적이지만 P-R은 모델 세트에 따라 성능 차이 존재\n",
    "\n",
    "3. 코사인 거리 응용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3. Classic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SVM(Support Vector Machine) : 지도학습 방법 중에서도 가장 뛰어난 알고리즘\n",
    "\n",
    "2. 로지스틱 회귀(Logistic Regression) : 머신러닝 영역에서 가장 기초적이고 가장 자주 사용하는 모델\n",
    " - 주로 분류 모델에 사용되는 모델\n",
    " - ① $ y = a + bx $ 에서 x의 범위는 -inf ~ inf, y의 범위는 0 ~ 1<br>\n",
    "   ② 누적확률의 오즈(성공확률 / 실패확률) = $ F(x) / (1-F(x)) $<br>\n",
    "   ③ 확률의 최대 1/0 = inf, 최소 0/1 = 0, 즉 0 <= odds <= inf ---> log를 취하면 -inf <= log(odds) <= inf<br> \n",
    "   ④ log(odds)를 $ y = a + bx $라 생각하고 회귀분석 방법 사용후 양변에 e승을 취함<br>\n",
    "   ⑤ odds = $ F(x) \\over 1 - F(x) $ = $ e^{a+bx} $\n",
    "3. 의사결정트리 - 하향식으로 샘플 데이터에 대해 트리 형태의 분류를 진행\n",
    " - 내부 노드(internal node) : 특성 혹은 속성\n",
    " - 잎 노드(leaf node) : 클래스\n",
    " - 가장 기초적인 지도학습 모델로 분류와 회귀에서 사용함.\n",
    " - 마케팅, 생물의약 영역에서 의사결정 괒어과 매우 유사해서 많이 사용함.\n",
    " - 의사결정트리의 앙상블 학습에 응용 - 랜덤 포레스트(random forest), 그래디언트 부스팅(gradient boosting decision tree)\n",
    " - 장점 : 간단하며 직관적이고 설명이 뛰어나다\n",
    " - NP문제로 휴리스틱 학습 방법을 통해 의사결정 트리를 찾음\n",
    " - 알고리즘 : ID3, C4.5, CART\n",
    " > ⓐ ID3 - 최대 정보 이득(empirical entropy) ( 샘플 집합 D, 클래스 수 K, H(D) 경험 엔트로피<br>\n",
    " > H(D) = 공식채우기<br>\n",
    " > ⓑ C4.5 - 최대 정보 이득비(information gain ratio)<br>\n",
    " > gr(D, A) = 공식채우기<br>\n",
    " > ⓒ CART - 최대 지니 계수 - 지니(Gini)는 데이터의 순도를 나타내고, 이진 트리 사용. 즉, 이진 분할 방법 사용<br>\n",
    " > Gini(D) = 공식채우기<br>\n",
    " - 가지치기 - 과적합 방지를 위한 방법(사전가지치기, 사후가지치기)\n",
    " > 사전 가지치기(pre-pruning) - 미리 트리가 자라는 것을 멈추는 방식<br>\n",
    " >> 깊이, 샘플 노드 수의 임계값, 정확도 향상에 미치는 임계값 -- 경험적 판단 필요 & 소적합의 위험 존재<br>\n",
    " > 사후 가지치기(post-pruning) - 이미 생선된 트리를 간략한 버전의 의사결정으로 만드는 방식\n",
    " >> 장점 : 높은 일반화 성능, 단점 : 계산시간이 오래 걸림<br>\n",
    " >> !!상향식 가지치기에 대한 정리 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6. Probability Graph Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7. Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 지도학습에서의 손실함수\n",
    " - 분류문제 손실함수\n",
    " > ⓐ 0-1손실 : 이진분류에서 가장 자연스러운 손실함수<br>\n",
    " > ⓑ 힌지(Hinge) 손실함수 : 0-1손실함수의 컨벡스 상계(Upper bound), 경사하강법 사용 X, Subgradient Descent Method 사용<br>\n",
    " > ⓒ 로지스틱(Logistic) 손실함수 : 특이점에 상대적으로 더 민감<br>\n",
    " > ⓓ 크로스 엔트로피(Cross Entropy) : 0-1 손실함수의 매끄러운 컨벡스 상계<br>\n",
    " - 회귀문제 손실함수\n",
    " > ⓐ 제곱 손실함수(Quadratic Loss Function) : 예측값 거리가 실제 거리와 멀수록 제곱 손실함수의 패널티 정도가 커지면서 특이점에 비교적 민감<br>\n",
    " > ⓑ 절댓값 손실함수(Absolute Loss Function) : 비교적 견고(Robust)하지만 f = y에서 미분할 수 없음.<br>\n",
    " > ⓒ 후버 손실함수(Huber Loss Function) : $ \\left | f - y \\right | $가 작을 때 제곱 손실 사용, $ \\left | f - y \\right | $가 클 때, 선형 손실을 사용\n",
    " \n",
    "2. 머신러닝에서의 최적화 문제\n",
    " - 강 전역최소자(strict global minimizer), 국소 최소자(local minimizer), 강 국소 최소자(strict local minimizer)\n",
    " - !!람다 0, 1 대입하면 x에서 y값\n",
    " - 컨벡스 함수(convex function)의 역모양이 컨케이브 함수(concave function)이다. 즉 컨벡스 함수에서 마이너스(-)를 붙이면 컨케이브 함수이다.\n",
    " - 컨벡스 함수 최적화 예시) 로지스틱 회귀모델, SVM, 선형회귀 등등\n",
    " - 논컨벡스 함수 최적화 예시) 행렬 인수분해(MF), 주성분분석(PCA), 딥러닝 모델 등\n",
    " \n",
    "3. 전통적인 최적화 알고리즘\n",
    " - 2가지 방식(직접법과 반복법)\n",
    " > 직접법의 목적함수의 조건 - ① 컨벡스 함수여야 한다. ② '닫힌 형식의 해(closed-form-solution)'<br>\n",
    " > 반복법의 2가지 방법 - ① 일차 방법(경사하강법) - 일차 테일러 전개 ② 이차 방법(뉴턴법)\n",
    " \n",
    "4. 경사하강법의 검증 방법\n",
    " - 다시 보기\n",
    " \n",
    "5. 확률적 경사하강법\n",
    " - 전통적 경사하강법 : 갱신할때마다 모든 데이터를 사용하기 때문에 계산시간이 길어진다.\n",
    " - 확률적 경사하강법(SGD, Stochastic Gradient Descent) : 단일 훈련 데이터의 손실을 사용해 평균손실에 근사하기 때문에 수렴 속도가 높음.\n",
    " - 미니배치 경사하강법(Mini-Batch Gradient Descent) : 단일 데이터가 아닌 소수의 데이터 사용\n",
    " > 주의사항 : ①데이터 선택의 갯수 : 행렬을 충분히 사용하기 위해 2의 거듭제곱을 사용 ② 매번 훈련데이터를 랜덤하게 배열 ③ 학습률은 큰 수에서 정체기에 들어서면 점차 작은 값으로\n",
    " \n",
    "6. 확률적 경사하강법의 가속\n",
    " - 확률적 경사하강법은 최적해가 아닌 골짜기나 안장점에 멈출 수 있고 해를 못찾는 경우가 발생할 수 있음.\n",
    " - 해결방법1 : 모멘텀 방법(관성유지), 해결방법2 : AdaGrad 방법(환경유지), 해결방법3 : Adam(관성유지 + 환경유지)\n",
    " \n",
    "7. L1 정규화와 희소성\n",
    " - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
